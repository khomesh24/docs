{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#test-subject-line","title":"test subject line","text":""},{"location":"Helm/helm/","title":"Helm","text":"<p>The package manager for Kubernetes</p>"},{"location":"Helm/helm/#manager-helm-repo","title":"Manager helm repo","text":"<p>Add new repo </p> <pre><code>helm repo add helm-public-repos &lt;repo url&gt; --username &lt;&gt; --password &lt;&gt;\n</code></pre> <p>List repo list <pre><code>helm repo list\n</code></pre></p> <p>Search helm in repo </p> <pre><code>helm search repo &lt;name&gt;\n</code></pre>"},{"location":"Helm/helm/#creating-a-new-helm-chart","title":"Creating a new helm chart","text":"<pre><code>helm create &lt;name&gt;\n</code></pre> <p>Its create a chart directory with file and directory used in charts.</p> <pre><code>new-chart\n\u251c\u2500\u2500 Chart.yaml                      # Information about chart\n\u251c\u2500\u2500 charts                          # Stores dependant charts \n\u251c\u2500\u2500 templates                       # Contains the resource defination of charts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 NOTES.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 _helpers.tpl\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 deployment.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hpa.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ingress.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 service.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 serviceaccount.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tests                       # Test file \n\u2502\u00a0\u00a0     \u2514\u2500\u2500 test-connection.yaml\n\u2514\u2500\u2500 values.yaml                     # Default values for var used in templates\n</code></pre>"},{"location":"Helm/helm/#render-helm-templates","title":"Render Helm Templates","text":"<p>locally render templates</p> <pre><code>helm templates &lt;name&gt; &lt;chart&gt;\n</code></pre>"},{"location":"Helm/helm/#install-charts","title":"Install Charts","text":"<p>Apply charts to k8s cluster. It basically render and then use <code>kubectl apply</code> the templates it generate. </p> <pre><code>helm install &lt;name&gt; &lt;chart path&gt;\n</code></pre>"},{"location":"Helm/helm/#list-helm","title":"List helm","text":"<p>List all helm that are applied to k8s cluster.</p> <pre><code>helm ls \n</code></pre>"},{"location":"Helm/helm/#upgrade-helm","title":"Upgrade Helm","text":"<p>Upgrade a release. When we change helm template or want to override new values, we run helm upgrade. This will generate new manifeast and run <code>kubectl apply</code>. </p> <p>Note: This could terminate all pod workload and recreate it that could cause downtime.</p>"},{"location":"Helm/helm/#uninstall-a-release","title":"Uninstall a release","text":"<p>Remove release </p> <pre><code>helm uninstall &lt;name&gt;\n</code></pre>"},{"location":"Helm/helm_dependency/","title":"Helm dependency","text":""},{"location":"Helm/helm_dependency/#helm-dependency","title":"Helm Dependency","text":"<p>There are lot of pre-define helm charts. So using dependencies we can directly import those charts and override the variables according to our needs.</p> <p>Declare helm charts </p> <pre><code># Charts.yaml\n\ndependencies:\n  - name: ngnix\n    version: \"1.2.3\"\n    repository: \"https://example.com/charts\"\n  - name: prometheus\n    version: \"4.4.5\"\n    repository: \"https://example2.com/charts\"\n</code></pre>"},{"location":"Helm/helm_dependency/#dependency-list","title":"Dependency list","text":"<pre><code>helm dependency ls\n</code></pre>"},{"location":"Helm/helm_dependency/#dependency-build","title":"Dependency build","text":"<p>Build is used to reconstruct a chart's dependencies to the state specified in the lock file. This will not re-negotiate dependencies, as 'helm dependency update' does. If no lock file is found, 'helm dependency build' will mirror the behavior of <code>helm dependency update</code>.</p> <pre><code>helm dependency build &lt;chart&gt;\n</code></pre>"},{"location":"Helm/helm_dependency/#dependency-update","title":"Dependency update","text":"<p>Build will construct chart in <code>charts</code> directory in parent chart. </p> <pre><code>helm dependency update &lt;chart&gt;\n</code></pre>"},{"location":"Kubernetes/Architecture/","title":"Kubernetes Architecture","text":"<p>Kubernetes mainly consist of Master and worker node. Master will host all control plane service where as worker host all the workload. We usualy need more that one master for HA and we can have n number of worker depending the the requirement.</p> <p></p>"},{"location":"Kubernetes/Architecture/#components","title":"Components","text":"<ul> <li> <p>Kube API server  The API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane. </p> </li> <li> <p>ETCd  Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.</p> </li> <li> <p>Scheduler  Control plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on.   Factors taken into account for scheduling decisions include: individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, and deadlines.</p> </li> <li> <p>Controller Manager  Control plane component that runs controller processes.  Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.  Some types of these controllers are:</p> <p>Node controller: Responsible for noticing and responding when nodes go down.  Job controller: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion.  EndpointSlice controller: Populates EndpointSlice objects (to provide a link between Services and Pods).  ServiceAccount controller: Create default ServiceAccounts for new namespaces. </p> </li> <li> <p>Cloud Controller manager  Cloud manager is use to interact with cloud provider. It help you perform scale in/out, route creation, managing cloud provided loadbalancer and manymore. </p> </li> <li> <p>Kubelet  Kubelet run on worker nodes. Kubelet manages kubernetes objects(pods/replicas/deployment, etc) on worker node. It interact with kubeapi service and fetch the details of pods or object schedule to run on that worker and ensure that the object is healthy.</p> </li> <li> <p>Kubeproxy  kube-proxy is a network proxy that runs on each worker node in your cluster. It maintains the network rule on the node to allow network communication to your Pods from network inside or outside of your cluster.  kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, kube-proxy forwards the traffic itself.</p> </li> </ul>"},{"location":"Kubernetes/Commands/","title":"Commands","text":"<pre><code>kubectl set image resource/name container_name=new_image\n</code></pre> <p>kubectl rollout status deployment/name</p> <p>kubectl rollout history deployment/name</p> <p>kubectl rollout undo deployment/name</p> <p>Switch namespace: </p> <pre><code>kubectl config set-context --current --namespace=&lt;insert-namespace-name-here&gt;\nkubectl config view --minify | grep namespace:\n</code></pre>"},{"location":"Kubernetes/Container%20scheduling/","title":"Container Scheduling","text":""},{"location":"Kubernetes/Container%20scheduling/#taints-and-tolerations","title":"Taints and Tolerations","text":"<p>Taints are applied on nodes, to avoid pod scheduling on that node. Toleration is applied on pods, this allows scheduler to schedule pod on tainted node. Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.</p> <p>In Addition, taint and toleration can make sure pods are not schedule in a specfic node but you cannot use this to schedule node on specific node.  </p> <p>There are three types of taints:</p> <p>NoSchedule: No schedule, but already running pod will keep running. NoExecute:  No schedule and i will evict exiting running nodes. PreferNoSchedule: Try to exclude the node from scheduling, but no guarantee. </p>"},{"location":"Kubernetes/Container%20scheduling/#command-to-set-taint","title":"Command to set taint","text":"<pre><code>kubectl traint node &lt;node name&gt; app=blue:NoSchedule\n</code></pre>"},{"location":"Kubernetes/Container%20scheduling/#pod-defination-to-set-tolerations","title":"Pod defination to set tolerations","text":"<pre><code>apiVersion: v1\nkind: pod\nmetadata:\n  name: pod_name\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n  tolerations:\n    - key: \"app\"\n      operator: \"Equal\"\n      value: \"blue\"\n      effect: \"NoSchedule\"\n</code></pre> <p>Note: All value in toleration need to be inside double quotes. </p>"},{"location":"Kubernetes/Container%20scheduling/#node-selector","title":"Node Selector","text":"<p>We can use nodeSelector in spec.containers.nodeSelector to assign pod to a specific node. We can simply use label to acchive this.</p>"},{"location":"Kubernetes/Container%20scheduling/#set-a-label-on-node","title":"Set a label on node","text":"<pre><code>kubectl label node &lt;node name&gt; node=blue\n</code></pre>"},{"location":"Kubernetes/Container%20scheduling/#set-nodeselector-on-the-pod-definations","title":"Set nodeSelector on the pod definations","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n  nodeSelector:\n    node: blue\n</code></pre> <p>But nodeSelector has a limition we cannot apply complex condition using this. To achive this we use Node Affinity.</p>"},{"location":"Kubernetes/Container%20scheduling/#node-affinity","title":"Node Affinity","text":"<p>It is same as nodeSelector but this is more expressive. And give more controller to user.</p> <p>There are two type of affinity.</p>"},{"location":"Kubernetes/Container%20scheduling/#requiredduringschedulingignoredduringexecution","title":"requiredDuringSchedulingIgnoredDuringExecution","text":"<p>Pod will be schedule only if requirement is met otherwise it will stuck in pending state. And after schedule if node label for some reason pod wont be evicted.</p>"},{"location":"Kubernetes/Container%20scheduling/#preferredduringschedulingignoredduringexecution","title":"preferredDuringSchedulingIgnoredDuringExecution","text":"<p>Scheduler will try to schedule on selected node, but if it not available or unable to schedule for some reason, pod will be schedule on some other node.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod_with_affinity\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: topology.kubernetes.io/zone\n            operator: In\n            values:\n            - antarctica-east1\n            - antarctica-west1\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n  containers:\n  - name: with-node-affinity\n    image: registry.k8s.io/pause:2.0\n</code></pre> <p>Refer Node Affinity for more info. </p>"},{"location":"Kubernetes/Management%20Techniques/","title":"Kubernetes Management Techniques","text":""},{"location":"Kubernetes/Management%20Techniques/#imperative-commands","title":"Imperative Commands","text":"<p>When using imperative commands, a user operates directly on live objects in a cluster. The user provides operations to the kubectl command as arguments or flags.</p> <p>This is the recommended way to get started or to run a one-off task in a cluster. Because this technique operates directly on live objects, it provides no history of previous configurations.</p> <pre><code>kubectl create deployment nginx --image nginx\nkubectl run nginx --image nginx\n</code></pre>"},{"location":"Kubernetes/Management%20Techniques/#imperative-object-configuration","title":"Imperative object configuration","text":"<p>In imperative object configuration, the kubectl command specifies the operation (create, replace, etc.), optional flags and at least one file name. The file specified must contain a full definition of the object in YAML or JSON format.</p> <pre><code>kubectl create -f nginx.yaml\nkubectl delete -f nginx.yaml -f redis.yaml\nkubectl replace -f nginx.yaml\n</code></pre>"},{"location":"Kubernetes/Management%20Techniques/#declarative-object-configuration","title":"Declarative object configuration","text":"<p>In declarative object configuration, a user operates on object configuration files stored locally, however the user does not define the operations to be taken on the files. Create, update, and delete operations are automatically detected per-object by kubectl. This enables working on directories, where different operations might be needed for different objects.</p> <pre><code>kubectl diff -f configs/\nkubectl apply -f configs/\n\n# Recursively process directories:\n\nkubectl diff -R -f configs/\nkubectl apply -R -f configs/\n</code></pre>"},{"location":"Kubernetes/Managing%20Applications/","title":"Application life cycle","text":""},{"location":"Kubernetes/Managing%20Applications/#rolling-updates-and-rollbacks","title":"Rolling Updates and Rollbacks","text":"<p>Rolling Update is a Stratagy use to update pods with downtime. We remove one old pod and replace it will newer one.</p> <p>It recommended modify the deployment defination file and then apply the new changes using kubectl apply -f . <p>Use below command to monitor.</p> <pre><code>kubectl rollout status deployment/name\nkubectl rollout history deployment/name\nkubectl rollout undo deployment/name\n</code></pre> <p>Different deployment stratagies we can use are below: </p> <ul> <li>Rolling deployment: replaces pods running the old version of the application with the new version, one by one, without downtime to the cluster.</li> <li>Recreate: terminates all the pods and replaces them with the new version.</li> <li>Ramped slow rollout: rolls out replicas of the new version, while in parallel, shutting down old replicas. </li> <li>Best-effort controlled rollout: specifies a \u201cmax unavailable\u201d parameter which indicates what percentage of existing pods can be unavailable during the upgrade, enabling the rollout to happen much more quickly.</li> <li>Canary deployment: uses a progressive delivery approach, with one version of the application serving most users, and another, newer version serving a small pool of test users. The test deployment is rolled out to more users if it is successful.</li> </ul> <p>There is also a very popular strategy called Blue_Green. Refer doc for more detailed explaination.</p>"},{"location":"Kubernetes/Managing%20Applications/#commands-vs-arguments","title":"Commands vs Arguments","text":"<p>Command override the entrypoint define in the container image and arguments is use to override CMD. This parameter cannot be changed once pod is created. </p>"},{"location":"Kubernetes/Managing%20Applications/#example","title":"Example:","text":"<p>Below is the docker file for image</p> <pre><code>FROM python:3.6-alpine\n\nRUN pip install flask\n\nCOPY . /opt/\n\nEXPOSE 8080\n\nWORKDIR /opt\n\nENTRYPOINT [\"python\", \"app.py\"]\n\nCMD [\"--color\", \"red\"]\n</code></pre> <p>Pod defination</p> <pre><code>apiVersion: v1 \nkind: Pod \nmetadata:\n  name: webapp-green\n  labels:\n      name: webapp-green \nspec:\n  containers:\n  - name: simple-webapp\n    image: webapp-color\n    command: [\"python\", \"app.py\"]\n    args: [\"--color\", \"pink\"]\n</code></pre> <p>args in pod defination will override the CMD in image. And we set command to [\"python3\", \"app.py\"] in defination then it will override entrypoint in image. So in our case when pod is created it will run python app,py --color pink</p>"},{"location":"Kubernetes/Managing%20Applications/#passing-env-to-pods","title":"Passing env to pods","text":"<p>Example 1: </p> <pre><code>spec:\n  containers:\n    - name: nginx\n      image: nginx\n      env:\n        - name: Key_name\n          value: Key_value\n</code></pre> <p>Example 2: Use all key-value pair in env</p> <pre><code>spec:\n  containers:\n    - name: nginx\n      image: nginx\n      envFrom:\n        - configMapRef:\n            name: &lt;configMap_name&gt;\n</code></pre> <p>Example 3: Use very specific key-value pair as env</p> <pre><code>spec:\n  containers:\n    - name: nginx\n      image: nginx\n      env:\n        - name: &lt;keyname&gt;\n          valueFrom:\n            configMapKeyRef:\n              name: &lt;configMap_name&gt;\n              key: &lt;key in config map&gt;\n</code></pre> <p>Example 4:</p> <pre><code>spec:\n  containers:\n    - name: nginx\n      image: nginx\n      envFrom:\n        - secretKeyRef:\n            name: &lt;secret_name&gt;\n</code></pre>"},{"location":"Kubernetes/Managing%20Applications/#configmaps","title":"ConfigMaps","text":"<p>Create a key value pair do store data and </p>"},{"location":"Kubernetes/Managing%20Applications/#secrets","title":"Secrets","text":""},{"location":"Kubernetes/Managing%20Applications/#encrying-secret-data-at-rest","title":"Encrying secret data at rest","text":""},{"location":"Kubernetes/Managing%20scheduler/","title":"Managing Scheduler","text":""},{"location":"Kubernetes/Managing%20scheduler/#scheduler","title":"Scheduler","text":"<p>Kubernetes has default scheduler in kube-system namespace. We can add a custom namespace depending on our requirement.</p> <p>To user custom scheduler you can use schedulerName parameter in pod specs.</p> <pre><code>apiVersion: v1 \nkind: Pod \nmetadata:\n  name: nginx \nspec:\n  containers:\n  - image: nginx\n    name: nginx\n  schedulerName: my-scheduler\n</code></pre>"},{"location":"Kubernetes/Managing%20scheduler/#scheduling-process","title":"Scheduling Process","text":"<p>Scheduling Queue  -&gt;  Filtering  -&gt; Scoring -&gt; Binding</p> <p>Scheduling Queue: Here all scheduling request are collected and sorted baised on priority. We use PrioritySort plugin to acchive this.</p> <p>Filtering: We filter out the node that are incapable to deploy given pod. It uses NodeResourceFit/NodeName/nodeUnschedulable plugins to filter out nodes.</p> <p>Scoring: This will give score to filtered node based on resource and resource stress on the node. More the stress lower the scores. It used NodeResourceFit/ImageLocality plugin to find the best fit.</p> <p>Binding: Depending on score binder will schedule node on best fit node. It uses binder plugin to schedule the node.</p> <p>Refer Advanced scheduling</p>"},{"location":"Kubernetes/Resources/","title":"Core Kubernetes Concept","text":""},{"location":"Kubernetes/Resources/#pods","title":"Pods","text":"<p>Pods are the smallest deployable units of computing that you can create and manage in Kubernetes. A Pod (as in a pod of whales or pea pod) is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. </p> <p>The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that isolate a container. Within a Pod's context, the individual applications may have further sub-isolations applied.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n    - name: nginx-container\n      image: nginx\n    - name: busybox-container\n      image: busybox\n</code></pre>"},{"location":"Kubernetes/Resources/#replicaset","title":"ReplicaSet","text":"<p>A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods.</p> <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: rs-app\n  labels:\n    app: rc-label\nspec:\n  template:\n    metadata:\n      name: pod-busybox\n      labels:\n        app: pod-label\n    spec:\n      containers:\n        - name: busybox\n          image: nginx\n  replicas: 2\n  selector:\n    matchLabels:\n      app: pod-label\n</code></pre> <p>ReplicaSet use label to identify the pods. It will use container defination in template to scale new pods.</p> <p>We can also use <code>kubectl scale</code> to modify replicas.</p> <p><code>kubectl scale --replicas &lt;num&gt; &lt;replicationset resource&gt;</code></p>"},{"location":"Kubernetes/Resources/#daemonset","title":"DaemonSet","text":"<p>DaemonSet is similar to ReplicaSet but it will maintain one replica on each nodes. When a node is added to cluster DaemonSet will create a replica on in and remove when node is deleted. This is mainly use for monitering and logging purposes. It used NodeAffinity to schedule and pods on node. </p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: elasticsearch\n  labels:\n    app: monitering\nspec:\n  template:\n    metadata:\n      labels:\n        app: pod-fluentd\n    spec:\n      containers:\n        - name: fluentd\n          image: fluentd\n  selector:\n    matchLabels:\n      app: pod-fluentd\n</code></pre>"},{"location":"Kubernetes/Resources/#deployment","title":"Deployment","text":"<p>Deployment is like a orchestration tool, we can used this manage pods/replicas/rolling update and other operations efficiently. </p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rs-app\n  labels:\n    app: rc-label\nspec:\n  template:\n    metadata:\n      name: pod-busybox\n      labels:\n        app: pod-label\n    spec:\n      containers:\n        - name: busybox\n          image: nginx\n  replicas: 2\n  selector:\n    matchLabels:\n      app: pod-label\n</code></pre>"},{"location":"Kubernetes/Resources/#services","title":"Services","text":"<p>Service is a method for exposing a network application that is running as one or more Pods in your cluster</p> <p>For example, consider a stateless image-processing backend which is running with 3 replicas. Those replicas are fungible\u2014frontends do not care which backend they use. While the actual Pods that compose the backend set may change, the frontend clients should not need to be aware of that, nor should they need to keep track of the set of backends themselves.</p> <p>The Service abstraction enables this decoupling. It used selector to find the endpoint.</p>"},{"location":"Kubernetes/Resources/#nodeport","title":"Nodeport","text":"<p>Nodeport uses system port to route the external traffic to pod.</p> <p>nodeport: System(worker node) port. Valid range: 30000-32767  port: Service port  targetport: pod port</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: Service name \nspec:\n  type: NodePort\n  ports:\n    - port:\n      nodePort:\n      targetPort: \n  selector:\n    &lt;label key&gt;: &lt;label value&gt; \n</code></pre>"},{"location":"Kubernetes/Resources/#clusterip","title":"ClusterIP","text":"<p>Here we create a virtual IP and map all pod of same type to it. We use labels to mapd pods. ClusterIp will loadbalance the request and route to any of the available pod.</p> <p><pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: Service name \nspec:\n  type: ClusterIP\n  ports:\n    - port:\n      targetPort: \n  selector:\n    &lt;label key&gt;: &lt;label value&gt; \n</code></pre>  Note: if type is not define that it will by default conside it as ClusterIP </p>"},{"location":"Kubernetes/Resources/#loadbalances","title":"Loadbalances","text":""},{"location":"Kubernetes/Resources/#namespaces","title":"Namespaces","text":"<p>Namespaces provides a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. </p> <p>Creating a namespace: </p> <p><pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: test-namespace\n</code></pre> <pre><code>kubectl create -f &lt;test-namespace.yaml&gt;\n</code></pre></p> <p>Switch namespace: </p> <pre><code>kubectl config set-context --current --namespace=&lt;insert-namespace-name-here&gt;\nkubectl config view --minify | grep namespace:\n</code></pre> <p>DNS entry maintain below pattern <code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local</code></p>"},{"location":"Kubernetes/Static_pods/","title":"Static Pods","text":"<p>Static Pod that are directly manage by kubelet daemon without the API server observing them.  Static Pods are always bound to one Kubelet on a specific node. </p> <p>The kubelet automatically tries to create a mirror Pod on the Kubernetes API server for each static Pod. This means that the Pods running on a node are visible on the API server, but cannot be controlled from there. The Pod names will be suffixed with the node hostname with a leading hyphen. </p> <p>Finding Static pods defination</p> <p>On the node look for --config in kubelet service file.</p> <pre><code>$ systemctl cat kubelet  | grep config\n[Unit]\nDescription=kubelet: The Kubernetes Node Agent\n\n[.....]\n\n[Service]\nEnvironment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-ku&gt;\nEnvironment=\"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\"                    &lt;&lt;&lt;&lt;&lt; \n# This is a file that \"kubeadm init\" and \"kubeadm join\" generates at runtime, populating&gt;\nEnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env\n# This is a file that the user can use for overrides of the kubelet args as a last resor&gt;\n# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBE&gt;\nEnvironmentFile=-/etc/default/kubelet\nExecStart=\nExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEAD&gt;\n</code></pre> <p>And in the --config file look for staticPodPath</p> <pre><code>$ grep ^static /var/lib/kubelet/config.yaml \nstaticPodPath: /etc/kubernetes/manifests\n</code></pre> <p>In above case, /etc/kubernetes/manifests is the directory where statis pod defination are stored. We can create our own pod defination in here and kubelet will start and manage this pod.</p>"},{"location":"Kubernetes/Storage/","title":"Kubernetes Storage","text":""},{"location":"Kubernetes/Storage/#persistent-volume","title":"Persistent volume","text":"<p>A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. </p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: volume\n  labels:\n    app: test-vol\nspec:\n  accessMode:\n    - ReadWriteOnce\n  capacity:\n    storage: 10Gi\n  storageClassName: &lt;storage class name&gt;\n  hostPath:\n    path: \"/mnt/data\"\n</code></pre>"},{"location":"Kubernetes/Storage/#persistent-volume-claim","title":"Persistent volume claim","text":"<p>A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany)</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: task-pv-claim\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi\n</code></pre>"},{"location":"Kubernetes/Storage/#using-pvc-in-pod","title":"Using PVC in Pod","text":"<p>Once you create a PVC use it in a POD definition file by specifying the PVC Claim name under persistentVolumeClaim section in the volumes section like this:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-pvc\n  labels:\n    app: pod-pvc\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - name: test-vol\n      mountPath: /mnt\n  volumes:\n  - name: test-vol\n    persistentVolumeClaim:\n      claimName: pvc-vol\n  volumeName: test-vol # when you want to map pvc to specific pv\n</code></pre> <p>The same is true for ReplicaSets or Deployments. Add this to the pod template section of a Deployment on ReplicaSet.</p>"},{"location":"Openshift/Managing%20Identify%20Provider/","title":"Identify Provider","text":""},{"location":"Openshift/Managing%20Identify%20Provider/#htpasswd","title":"HTPasswd","text":""},{"location":"Openshift/Managing%20Identify%20Provider/#create-htpasswd-file","title":"Create htpasswd file","text":"<p>httpd-tools packages provides htpasswd. </p> <pre><code>htpasswd &lt;flags&gt; &lt;path to file&gt; &lt;user&gt; &lt;passwd&gt;\n\nFlags:\n\n-c Create new htpasswd file, Only need to use it once.\n-b Dont prompt for the passwd take it from cmd line\n-B Force bcrypt encryption to store passwd\n-m Use md5 to store passwd (default)\n-D Delete user from the file\n-v verify passwd from user \n</code></pre> <p>Examples:</p> <pre><code>htpasswd -c -B -b /tmp/htpasswd admin admin_pass\nhtpasswd -B -b /tmp/htpasswd dev dev_pass\nhtpasswd -b /tmp/htpasswd operator ops_pass\n</code></pre>"},{"location":"Openshift/Managing%20Identify%20Provider/#upload-htpasswd-file-as-secret","title":"Upload htpasswd file as secret","text":"<pre><code>oc create secret generic localuser-srt --from-file htpasswd=&lt;path to htpasswd&gt; -n openshift-config\n</code></pre>"},{"location":"Openshift/Managing%20Identify%20Provider/#modify-oauth-resource-and-add-identity-provider","title":"Modify OAuth resource and add identity provider","text":"<pre><code>oc get oauth cluster -o yaml &gt; oauth.yaml\n</code></pre> <p>Add below below section in oauth spec</p> <pre><code>...\nspec:\n  identityProviders :\n    - name: localuser\n      type: HTPassed                &lt;&lt;--\n      mappingMethod: claim          &lt;&lt;--\n      htpasswd:\n        fileData:\n          name: localuser-srt       &lt;&lt;-- secret name\n</code></pre> <p>update the resource defination</p> <pre><code>oc replace -f oauth.yaml \n</code></pre> <p>Moniter if pod in openshift-authentication to read new configuration</p> <pre><code>oc get pod -n openshift-authentication -w\n</code></pre> <p>Verify if you are able to login</p> <pre><code>oc login -u admin -p admin_pass\noc whoami\n</code></pre>"},{"location":"Openshift/Managing%20Identify%20Provider/#set-admin-role-to-users","title":"Set admin role to users","text":"<p>Set cluster-admin role to admin user</p> <pre><code>oc adm policy add-cluster-role-to-user  cluster-admin admin\n</code></pre> <p>List current users</p> <pre><code>oc get users\n</code></pre> <p>List current identifies</p> <pre><code>oc get identity\n</code></pre>"},{"location":"Openshift/Managing%20Identify%20Provider/#extract-htpassd-to-addremove-user","title":"Extract htpassd to add/remove user","text":"<p>Extract secret file</p> <pre><code>oc extract secret/localuser-srt -n openshift-config --to &lt;dest path&gt; --confirm\n</code></pre> <p>Add manager user and remove operator user</p> <pre><code>htpasswd -b -B /tmp/htpasswd manager mgr_pass\nhtpassed -D /tmp/htpasswd operator \n</code></pre> <p>Upload the file back</p> <pre><code>oc set data secret/localuser-srt --from-file htpasswd=&lt;path to htpasswd&gt; -n openshift-config\n</code></pre> <p>Delete identity and user resource </p> <pre><code>oc delete identity \"localuser:manager\"\noc delete user \"localuser:manager\"\n</code></pre>"},{"location":"Openshift/RBAC/","title":"Role-based Access Control (RBAC)","text":"<p>Role-based access control (RBAC) is a technique for managing access to resources in a computer system. In Red Hat OpenShift, RBAC determines if a user can perform certain actions within the cluster or project. There are two types of roles that can be used depending on the user's level of responsibility: cluster and local.</p>"},{"location":"Openshift/RBAC/#rbac-objects","title":"RBAC objects","text":"<ul> <li>Rule  Use to allowed action for object or grp of objects</li> <li>Role  Set of rule assigned to user or grp. User or grp can be assign to multiple role</li> <li>Binding  Assign role to user/grp. </li> </ul>"},{"location":"Openshift/RBAC/#rbac-scope","title":"RBAC scope","text":"<ul> <li>Cluster Role User or group has cluster level role</li> <li>Local Role User or group has project/namespace level role  </li> </ul>"},{"location":"Openshift/RBAC/#manage-rbac","title":"Manage RBAC","text":"<p>For cluster role we need to user oc adm policy and for local role we can user oc policy. </p> <p>To add a cluster role to a user, use the add-cluster-role-to-user subcommand: <pre><code>oc adm policy add-cluster-role-to-user cluster-role username\n</code></pre></p> <p>For example, to change a regular user to a cluster administrator, use the following command:</p> <pre><code>oc adm policy add-cluster-role-to-user cluster-admin username\n</code></pre> <p>To remove a cluster role from a user, use the remove-cluster-role-from-user subcommand:</p> <pre><code>oc adm policy remove-cluster-role-from-user cluster-role username\n</code></pre> <p>For example, to change a cluster administrator to a regular user, use the following command:</p> <pre><code>oc adm policy remove-cluster-role-from-user cluster-admin username\n</code></pre> <p>Rules are defined by an action and a resource. For example, the create user rule is part of the cluster-admin role.</p> <p>You can use the oc adm policy who-can command to determine if a user can execute an action on a resource. For example:</p> <pre><code>oc adm policy who-can delete user\n</code></pre>"},{"location":"Openshift/RBAC/#default-roles","title":"Default roles","text":"<ul> <li>admin  Project admin.</li> <li>basic-user Project read access.</li> <li>cluster-admin Cluster admin with access to all resource of cluster.</li> <li>cluster-status User you can get cluster status information.</li> <li>edit Users with this role can create, change, and delete common application resources from the project, such as services and deployments. These users cannot act on management resources such as limit ranges and quotas, and cannot manage access permissions to the project.</li> <li>view User can only view resources.</li> <li>self-provisioner User can create new project. This is a cluster role.</li> </ul>"},{"location":"Openshift/RBAC/#user-types","title":"User Types","text":"<ul> <li>Local Users Regular user that can login via cli or dashboard.</li> <li> <p>System Users System users are defined automatically when the infrastructure is defined. System users include a cluster administrator (with access to everything), a per-node user, users for routers and registries, and various others. An anonymous system user is used by default for unauthenticated requests. Examples of system users include: system:admin, system:openshift-registry, and system:node:node1.example.com.</p> </li> <li> <p>Service Accounts These are special system users associated with projects. Some service account users are created automatically when the project is first created. Project administrators can create more for the purpose of defining access to the contents of each project. Service accounts are often used to give extra privileges to pods or deployments. Service accounts are represented with the ServiceAccount object. Examples of service account users include system:serviceaccount:default:deployer and system:serviceaccount:foo:builder.</p> </li> </ul>"},{"location":"Openshift/RBAC/#sample-commands","title":"Sample commands:","text":"<pre><code>oc get clusterrolebinding -o wide\n\noc describe clusterrolebindings self-provisioners\n\noc adm groups new &lt;grp name&gt;\n\noc adm groups new-users &lt;grp name&gt; &lt;user name&gt;\n\noc get groups -o wide \n\noc get rolebindings -o wide\n\noc adm policy add-cluster-role-to-group  --rolebinding-name self-provisioners  self-provisioner system:authenticated:oauth\n</code></pre>"},{"location":"general%20article/Minikube/","title":"Minikube","text":""},{"location":"general%20article/Minikube/#installation","title":"Installation","text":""},{"location":"general%20article/Minikube/#start-cluster","title":"Start cluster","text":"<pre><code>minikube start [flags]\n</code></pre> <p>List of all Flags</p>"},{"location":"general%20article/Minikube/#manage-cluster","title":"Manage Cluster","text":"<p>Pause Kubernetes without impacting deployed applications: <pre><code>minikube pause\n</code></pre></p> <p>Unpause a paused instance: <pre><code>minikube unpause\n</code></pre></p> <p>Halt the cluster: <pre><code>minikube stop\n</code></pre></p> <p>Change the default memory limit (requires a restart): <pre><code>minikube config set memory 9001\n</code></pre></p> <p>Browse the catalog of easily installed Kubernetes services: <pre><code>minikube addons list\n</code></pre></p> <p>Create a second cluster running an older Kubernetes release: <pre><code>minikube start -p aged --kubernetes-version=v1.16.1\n</code></pre></p> <p>Delete all of the minikube clusters: <pre><code>minikube delete --all\n</code></pre></p>"},{"location":"general%20article/Minikube/#interact","title":"Interact","text":"<p>We can use kubectl to access cluster</p> <pre><code>kubectl get all\n</code></pre> <p>Alternatively, you can use minikube to interact with cluster</p> <pre><code>minikube kubectl -- get pods\n</code></pre> <p>Create a alias to make things easier</p> <pre><code>alias kubectl='minikube kubectl'\n</code></pre> <p>To access kubernetes dashboard </p> <pre><code>minikube dashboard\n</code></pre>"},{"location":"general%20article/Minikube/#deploy-applications","title":"Deploy Applications","text":""},{"location":"general%20article/Minikube/#service","title":"Service","text":"<p>Create a sample deployment and expose it on port 8080:</p> <pre><code>kubectl create deployment hello-minikube --image=kicbase/echo-server:1.0\nkubectl expose deployment hello-minikube --type=NodePort --port=8080\n</code></pre> <p>It may take a moment, but your deployment will soon show up when you run:</p> <pre><code>kubectl get services hello-minikube\n</code></pre> <p>The easiest way to access this service is to let minikube launch a web browser for you:</p> <pre><code>minikube service hello-minikube\n</code></pre> <p>Alternatively, use kubectl to forward the port:</p> <p><pre><code>kubectl port-forward service/hello-minikube 7080:8080\n</code></pre> Tada! Your application is now available at http://localhost:7080/.</p>"},{"location":"general%20article/Minikube/#loadbalancer","title":"LoadBalancer","text":"<p>To access a LoadBalancer deployment, use the \u201cminikube tunnel\u201d command. Here is an example deployment:</p> <pre><code>kubectl create deployment balanced --image=kicbase/echo-server:1.0\nkubectl expose deployment balanced --type=LoadBalancer --port=8080\n</code></pre> <p>In another window, start the tunnel to create a routable IP for the \u2018balanced\u2019 deployment:</p> <p><pre><code>minikube tunnel\n</code></pre> To find the routable IP, run this command and examine the EXTERNAL-IP column:</p> <pre><code>kubectl get services balanced\n</code></pre> <p>Your deployment is now available at :8080"},{"location":"general%20article/Minikube/#ingress","title":"Ingress","text":"<p>Enable ingress addon:</p> <p>minikube addons enable ingress The following example creates simple echo-server services and an Ingress object to route to these services.</p> <pre><code>kind: Pod\napiVersion: v1\nmetadata:\n  name: foo-app\n  labels:\n    app: foo\nspec:\n  containers:\n    - name: foo-app\n      image: 'kicbase/echo-server:1.0'\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: foo-service\nspec:\n  selector:\n    app: foo\n  ports:\n    - port: 8080\n---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: bar-app\n  labels:\n    app: bar\nspec:\n  containers:\n    - name: bar-app\n      image: 'kicbase/echo-server:1.0'\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: bar-service\nspec:\n  selector:\n    app: bar\n  ports:\n    - port: 8080\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\nspec:\n  rules:\n    - http:\n        paths:\n          - pathType: Prefix\n            path: /foo\n            backend:\n              service:\n                name: foo-service\n                port:\n                  number: 8080\n          - pathType: Prefix\n            path: /bar\n            backend:\n              service:\n                name: bar-service\n                port:\n                  number: 8080\n</code></pre> <p>Apply the contents</p> <pre><code>kubectl apply -f &lt;resource defination.yaml&gt;\n</code></pre> <p>Wait for ingress address</p> <pre><code>kubectl get ingress\n\nNAME              CLASS   HOSTS   ADDRESS          PORTS   AGE\nexample-ingress   nginx   *       &lt;your_ip_here&gt;   80      5m45s\n</code></pre>"},{"location":"general%20article/Minikube/#note-for-docker-desktop-users","title":"Note for Docker Desktop Users:","text":"<p>To get ingress to work you\u2019ll need to open a new terminal window and run minikube tunnel and in the following step use 127.0.0.1 in place of . <p>Now verify that the ingress works</p> <pre><code>$ curl &lt;ip_from_above&gt;/foo\nRequest served by foo-app\n...\n</code></pre> <pre><code>$ curl &lt;ip_from_above&gt;/bar\nRequest served by bar-app\n...\n</code></pre>"},{"location":"general%20article/Setup%20VNC/","title":"VNC setup","text":""},{"location":"general%20article/Setup%20VNC/#disabling-wayland-display-manager-and-enabling-xorg","title":"Disabling Wayland Display Manager and Enabling X.org","text":"<p>In /etc/gdm/custom.conf uncomment WaylandEnable=false </p> <pre><code># GDM configuration storage\n\n[daemon]\n# Uncoment the line below to force the login screen to use Xorg\nWaylandEnable=false\n\n[security]\n\n[xdmcp]\n\n[chooser]\n\n[debug]\n# Uncomment the line below to turn on debugging\n#Enable=true\n</code></pre>"},{"location":"general%20article/Setup%20VNC/#install-vnc-server-in-rhel-8","title":"Install VNC Server in RHEL 8","text":"<p>Install tigervnc packages</p> <pre><code>dnf install tigervnc-server tigervnc-server-module\n</code></pre> <p>set vncpasswd with root user</p> <pre><code>vncpasswd\n</code></pre>"},{"location":"general%20article/Setup%20VNC/#configure-vnc-service","title":"Configure VNC service","text":"<p>Create systemd file</p> <pre><code>vi /etc/systemd/system/vncserver@.service\n\n[Unit]\nDescription=Remote desktop service (VNC)\nAfter=syslog.target network.target\n\n[Service]\nType=forking\nWorkingDirectory=/root\nUser=root\nGroup=root\n\nPIDFile=/root/.vnc/%H%i.pid\n\nExecStartPre=/bin/sh -c '/usr/bin/vncserver -kill %i &gt; /dev/null 2&gt;&amp;1 || :'\nExecStart=/usr/bin/vncserver -autokill %i\nExecStop=/usr/bin/vncserver -kill %i\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Disable selinux</p> <pre><code>setenforce 0\nsed -i 's/enforcing/disabled/g' /etc/selinux/config\n</code></pre> <p>Start vnc service</p> <pre><code>systemctl daemon-reload\nsystemctl start vncserver@:1\nsystemctl status vncserver@:1\nsystemctl enable vncserver@:1\n</code></pre> <p>Open firewall for vnc</p> <p>Identify the vnc port and add port in firewall</p> <pre><code>netstat -tlnp \n\nfirewall-cmd --permanent --add-port=&lt;port&gt;/tcp\nfirewall-cmd --reload\n</code></pre>"},{"location":"general%20article/VM%20from%20Cloud%20image/","title":"Using Cloud image to spin virtual machines","text":""},{"location":"general%20article/VM%20from%20Cloud%20image/#download-the-cloud-image-on-a-local-system","title":"Download the cloud image on a local system","text":"<pre><code>curl -O \"URL\"\n</code></pre>"},{"location":"general%20article/VM%20from%20Cloud%20image/#create-a-empty-qcow2-image","title":"Create a empty qcow2 image","text":"<pre><code>qemu-img create -f qcow2 ubuntu-bionic.qcow2 40G\n</code></pre>"},{"location":"general%20article/VM%20from%20Cloud%20image/#copy-the-image-into-empty-qcow2-image","title":"Copy the image into empty qcow2 image","text":"<pre><code>virt-resize --expand /dev/sda1 &lt;Cloud image&gt;  ubuntu-bionic.qcow2\n</code></pre> <p>Note: Use virt-filesystems command to find the root disk of cloud image.</p> <pre><code>virt-filesystems --csv --long --no-title -a &lt;Cloud image&gt;\n</code></pre>"},{"location":"general%20article/VM%20from%20Cloud%20image/#set-the-root-password-and-remove-cloud-init-package","title":"Set the root password and remove cloud-init package","text":"<pre><code>virt-customize -a ubuntu-bionic.qcow2 --root-password password:passwd --uninstall cloud-init \\\n        --hostname ubuntu \\\n        --run-command \"systemctl disable cloud-init cloud-config cloud-final cloud-init-local\" \\\n        --run-command \"echo 'PermitRootLogin yes' &gt; /etc/ssh/sshd_config.d/99-root-login.conf\" \\\n        --run-command \"mkdir -p /root/.ssh; chmod 0700 /root/.ssh\" \\\n        --run-command \"ssh-keygen -f /root/.ssh/id_rsa -N ''\" \\\n        --selinux-relabel \n</code></pre>"},{"location":"general%20article/VM%20from%20Cloud%20image/#use-new-image-to-spin-up-a-vm","title":"Use new image to spin up a VM","text":"<pre><code>virt-install --ram 12000 --vcpus 4 --disk path=ubuntu-bionic.qcow2,device=disk,bus=virtio,format=qcow2 --import --noautoconsole --vnc --network network:default --name ubuntu\n</code></pre>"},{"location":"general%20article/prometheus-grafana/","title":"Prometheus and Grafana for monitering","text":""},{"location":"general%20article/prometheus-grafana/#install-prometheus","title":"Install Prometheus","text":"<p>Download prometheus binaries from Download page</p>"},{"location":"general%20article/prometheus-grafana/#setup-prometheus-binaries","title":"Setup Prometheus Binaries","text":"<pre><code>wget https://github.com/prometheus/prometheus/releases/download/v2.44.0/prometheus-2.44.0.linux-amd64.tar.gz\ntar -xvf prometheus-2.44.0.linux-amd64.tar.gz\n</code></pre> <pre><code>useradd --no-create-home --shell /bin/false prometheus\nmkdir /etc/prometheus\nmkdir /var/lib/prometheus\n</code></pre> <pre><code>cp prometheus-2.44.0.linux-amd64/prometheus prometheus-2.44.0.linux-amd64/promtool /usr/local/bin/\ncp -r prometheus-2.44.0.linux-amd64/consoles /etc/prometheus/ \ncp -r prometheus-2.44.0.linux-amd64/console_libraries/ /etc/prometheus/\ncp prometheus.yml /etc/prometheus/\n</code></pre> <pre><code>chown prometheus:prometheus /etc/prometheus\nchown prometheus:prometheus /var/lib/prometheus\n</code></pre>"},{"location":"general%20article/prometheus-grafana/#setup-prometheus-configuration","title":"Setup Prometheus Configuration","text":"<p>All the prometheus configurations should be present in /etc/prometheus/prometheus.yml file.</p> <p>Create the prometheus.yml file.</p> <pre><code>vi /etc/prometheus/prometheus.yml\n</code></pre> <p>Copy the following contents to the prometheus.yml file.</p> <pre><code>global:\n  scrape_interval: 10s\n\nscrape_configs:\n  - job_name: 'prometheus'\n    scrape_interval: 5s\n    static_configs:\n      - targets: ['localhost:9090']\n</code></pre> <p>Change the ownership of the file to prometheus user.</p> <pre><code>chown prometheus:prometheus /etc/prometheus/prometheus.yml\n</code></pre>"},{"location":"general%20article/prometheus-grafana/#setup-prometheus-service-file","title":"Setup Prometheus Service File","text":"<p>Create a prometheus service file</p> <pre><code>vi /etc/systemd/system/prometheus.service\n</code></pre> <p>Copy the following content to the file</p> <pre><code>[Unit]\nDescription=Prometheus\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nUser=prometheus\nGroup=prometheus\nType=simple\nExecStart=/usr/local/bin/prometheus \\\n    --config.file /etc/prometheus/prometheus.yml \\\n    --storage.tsdb.path /var/lib/prometheus/ \\\n    --web.console.templates=/etc/prometheus/consoles \\\n    --web.console.libraries=/etc/prometheus/console_libraries\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Reload the systemd service to register the prometheus service and start the prometheus service</p> <pre><code>systemctl daemon-reload\nsystemctl start prometheus\n\nsystemctl status prometheus\n</code></pre> <pre><code>firewall-cmd --add-port 9090/udp --permanent\nfirewall-cmd --add-port 9090/tcp --permanent\nfirewall-cmd --reload\n</code></pre>"},{"location":"general%20article/prometheus-grafana/#access-prometheus-web-ui","title":"Access Prometheus Web UI","text":"<p>Now you will be able to access the prometheus UI on 9090 port of the prometheus server.</p> <pre><code>http://&lt;prometheus-ip&gt;:9090/graph\n</code></pre>"},{"location":"general%20article/prometheus-grafana/#install-node-exportor","title":"Install Node Exportor","text":""},{"location":"general%20article/prometheus-grafana/#setup-node-exporter-binary","title":"Setup Node Exporter Binary","text":"<p>Download the latest node exporter package. Download page</p> <pre><code>cd /tmp\nwget https://github.com/prometheus/node_exporter/releases/download/v1.6.0/node_exporter-1.6.0.linux-amd64.tar.gz\n</code></pre> <p>Unpack the tarball</p> <pre><code>tar -xvf node_exporter-0.18.1.linux-amd64.tar.gz\n</code></pre> <p>Move the node export binary to /usr/local/bin</p> <pre><code>sudo mv node_exporter-0.18.1.linux-amd64/node_exporter /usr/local/bin/\n</code></pre>"},{"location":"general%20article/prometheus-grafana/#create-a-custom-node-exporter-service","title":"Create a Custom Node Exporter Service","text":"<p>Create a node_exporter user to run the node exporter service.</p> <pre><code>sudo useradd -rs /bin/false node_exporter\n</code></pre> <p>Create a node_exporter service file under systemd.</p> <pre><code>vi /etc/systemd/system/node_exporter.service\n</code></pre> <p>Add the following service file content to the service file and save it.</p> <pre><code>[Unit]\nDescription=Node Exporter\nAfter=network.target\n\n[Service]\nUser=node_exporter\nGroup=node_exporter\nType=simple\nExecStart=/usr/local/bin/node_exporter\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Reload the system daemon and star the node exporter service.</p> <pre><code>systemctl daemon-reload\nsystemctl start node_exporter\n</code></pre> <p>check the node exporter status to make sure it is running in the active state.</p> <pre><code>systemctl status node_exporter\n</code></pre> <p>Enable the node exporter service to the system startup.</p> <pre><code>systemctl enable node_exporter\n</code></pre> <p>Now, node exporter would be exporting metrics on port 9100. </p> <p>You can see all the server metrics by visiting your server URL on /metrics as shown below.</p> <p>http://:9100/metrics"},{"location":"general%20article/prometheus-grafana/#install-grafana","title":"Install Grafana","text":""},{"location":"general%20article/prometheus-grafana/#download-install-grafana-binary","title":"Download &amp; install Grafana binary","text":"<pre><code>yum install -y https://dl.grafana.com/oss/release/grafana-9.5.2-1.x86_64.rpm\n</code></pre> <p>Run the following to add the above service unit and start grafana.</p> <pre><code>systemctl daemon-reload\nsystemctl start grafana-server\nsystemctl status grafana-server\nsystemctl enable grafana-server.service\nsystemctl status grafana-server\n</code></pre>"},{"location":"general%20article/prometheus-grafana/#grafana-access","title":"Grafana access","text":"<p>Let\u2019s open port 3000 to access the UI @ http://:3000/login <p>The default user &amp; password is \u2018admin\u2019 &amp; \u2018admin\u2019.</p>"},{"location":"general%20article/prometheus-grafana/#visualize-prometheus-metrics-in-grafana","title":"Visualize Prometheus Metrics In Grafana","text":""},{"location":"general%20article/terraform/","title":"Terraform","text":"<p>Terraform is a infrastructure as code tool use to build, change and update the infrastracture resources. This includes low-level components like compute instances, storage, and networking; and high-level components like DNS entries and SaaS features.&lt;br&gt;</p>"},{"location":"general%20article/terraform/#core-workflow","title":"Core Workflow","text":"<p>Initialize prepares the working directory so Terraform can run the configuration.</p> <pre><code>terraform init\n</code></pre> <p>Plan lets you preview any changes before you apply them.</p> <pre><code>terraform plan\n</code></pre> <p>Apply executes the changes defined by your Terraform configuration to create, update, or destroy resources. </p> <ul> <li> <p>Lock your project's state, so that no other instances of Terraform will attempt to modify your state or apply changes to your resources. If Terraform detects an existing lock file (.terraform.tfstate.lock.info), it will report an error and exit.</p> </li> <li> <p>Create a plan, and wait for you to approve it. Alternatively, you can provide a saved speculative plan created with the terraform plan command, in which case Terraform will not prompt for approval.</p> </li> <li> <p>Execute the steps defined in the plan using the providers you installed when you initialized your configuration. Terraform executes steps in parallel when possible, and sequentially when one resource depends on another.</p> </li> <li> <p>Update your project's state file with a snapshot of the current state of your resources.</p> </li> <li> <p>Unlock the state file.</p> </li> </ul> <pre><code>terraform apply\n</code></pre> <p>Refer docs for detailed apply actions</p> <p>Destroy will let you delete the resource provisioned</p> <pre><code>terraform destroy\n</code></pre>"},{"location":"general%20article/terraform/#variables","title":"Variables","text":"<p>We can use variable in terraform to make resource defination more composable and reuseable.</p> <p>Variable can we define like below, It variable block has three main attributes, type, description, default. </p> <pre><code>variable \"filename\" {\n        type: string\n        description: \"Path of the file to be created\"\n        default = \"/tmp/test-khomesh.file\"\n}\nvariable \"permission\" {\n        type: string\n        description: \"Permission need on file created\"\n        default = \"0777\"\n}\nvariable \"content\" {\n        type: string\n        description: \"Content of the file\"\n        default = \"hello \\nKhomesh \\nBye\"\n}\n</code></pre> <p>And refer like  <pre><code>resource \"local_file\" test {\n        filename = var.filename\n        file_permission = var.permission\n        content = var.content\n}\n</code></pre></p>"},{"location":"general%20article/terraform/#below-are-types-of-varaibles","title":"Below are types of varaibles:","text":"<ul> <li>string</li> <li>number</li> <li>bool</li> <li>list Can have multiple value in a sequence, like [\"foo\", \"bar\"]. But List need all values to be same type.</li> </ul> <p>example: <pre><code>variable pet {\n    type = list([string])\n    default = [\"cat\", \"dog\", \"turtle\"]\n}\n</code></pre></p> <ul> <li>set a collection of unique values that do not have any secondary identifiers or ordering.</li> </ul> <p>example:  <pre><code>variable fruits {\n    type = set(string)\n    default = [\"mango\", \"apple\", \"banana\"]\n}\n</code></pre></p> <ul> <li>map  a collection of values where each is identified by a string label.</li> </ul> <p>example:  <pre><code>variable dogs {\n    type = map(string)\n    default = {\n        \"name\" = \"rusty\"\n        \"color\" = \"brown\"\n    }\n}\n</code></pre></p> <ul> <li>object a collection of named attributes that each have their own type. </li> </ul> <p>example:  <pre><code>variable \"person\" {\n    type = object({\n        name = string\n        age = number\n        Country = string\n    })\n    default = object({\n        name = \"Khomesh\"\n        age = 26\n        Country = \"India\"\n    })\n}\n</code></pre> * tuple Like List tuples have multiple value in a sequence, like [\"foo\", \"bar\", 3]. The difference between Tuple and list is that tuple can have different type of values, while list need same type of values.</p> <p>example: <pre><code>variable test {\n    type = tuple([string, number, bool])\n    default = [\"cat\", 3, true]\n}\n</code></pre></p>"},{"location":"general%20article/terraform/#assigning-variables","title":"Assigning variables","text":"<p>There are number of ways to assign a variables</p> <ul> <li>environment variables </li> </ul> <pre><code>export TF_VAR_&lt;variable name&gt;=\"Value\"\nexport TF_VAR_filename = \"/tmp/test.txt\"\nexport TF_VAR_permission = \"0755\"\n\nterraform apply\n</code></pre> <ul> <li>*.tfvars  file name can be anything but end with .tfvars. </li> </ul> <pre><code>$ cat anything.tfvars\n\nfilename = \"/tmp/test.txt\"\npermission = \"0755\"\n</code></pre> <ul> <li>*.auto.tfvars Same as tfvars but loaded automatically</li> <li>-var or -var-file We can use -var flag in apply command to pass the variables</li> </ul> <pre><code>terraform apply -var \"filename=/root/pets.txt\" -var \"conten=we love pets!\"\n</code></pre>"},{"location":"general%20article/terraform/#variable-precedence","title":"Variable Precedence:","text":"<ul> <li>-var or -var-files    \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0    Highest precedence</li> <li>*.auto.tfvars \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |</li> <li>*.tfvars \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 V</li> <li>environment variables     Lowest precedence</li> </ul>"}]}